{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'module://matplotlib_inline.backend_inline'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ.pop('MPLBACKEND', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |█-------------------------------------------------| 1.4% Complete Loss: 2.2942 Acc: 10.00% ET: 00:02:13 ETA: 02:17:13"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     67\u001b[39m t_batch = t_train[batch_mask]\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# 기울기 계산\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m grad = \u001b[43mnetwork\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# grad = network.gradient(x_batch, t_batch) # 성능 개선판!\u001b[39;00m\n\u001b[32m     72\u001b[39m \n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# 매개변수 갱신\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mW1\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mb1\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mW2\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mb2\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/VisualStudioCode/DeepLearning/ch03/two_layer_net.py:52\u001b[39m, in \u001b[36mTwoLayerNet.numerical_gradient\u001b[39m\u001b[34m(self, x, t)\u001b[39m\n\u001b[32m     49\u001b[39m loss_W = \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28mself\u001b[39m.loss(x, t)\n\u001b[32m     51\u001b[39m grads = {}\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m grads[\u001b[33m\"\u001b[39m\u001b[33mW1\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_W\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mW1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m grads[\u001b[33m\"\u001b[39m\u001b[33mb1\u001b[39m\u001b[33m\"\u001b[39m] = numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m.params[\u001b[33m\"\u001b[39m\u001b[33mb1\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     54\u001b[39m grads[\u001b[33m\"\u001b[39m\u001b[33mW2\u001b[39m\u001b[33m\"\u001b[39m] = numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m.params[\u001b[33m\"\u001b[39m\u001b[33mW2\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/VisualStudioCode/DeepLearning/ch03/../common/gradient.py:47\u001b[39m, in \u001b[36mnumerical_gradient\u001b[39m\u001b[34m(f, x)\u001b[39m\n\u001b[32m     44\u001b[39m fxh1 = f(x)  \u001b[38;5;66;03m# f(x+h)\u001b[39;00m\n\u001b[32m     46\u001b[39m x[idx] = tmp_val - h\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m fxh2 = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# f(x-h)\u001b[39;00m\n\u001b[32m     48\u001b[39m grad[idx] = (fxh1 - fxh2) / (\u001b[32m2\u001b[39m * h)\n\u001b[32m     50\u001b[39m x[idx] = tmp_val  \u001b[38;5;66;03m# 값 복원\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/VisualStudioCode/DeepLearning/ch03/two_layer_net.py:49\u001b[39m, in \u001b[36mTwoLayerNet.numerical_gradient.<locals>.<lambda>\u001b[39m\u001b[34m(W)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnumerical_gradient\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     loss_W = \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     grads = {}\n\u001b[32m     52\u001b[39m     grads[\u001b[33m\"\u001b[39m\u001b[33mW1\u001b[39m\u001b[33m\"\u001b[39m] = numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m.params[\u001b[33m\"\u001b[39m\u001b[33mW1\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/VisualStudioCode/DeepLearning/ch03/two_layer_net.py:35\u001b[39m, in \u001b[36mTwoLayerNet.loss\u001b[39m\u001b[34m(self, x, t)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cross_entropy_error(y, t)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/VisualStudioCode/DeepLearning/ch03/two_layer_net.py:26\u001b[39m, in \u001b[36mTwoLayerNet.predict\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     23\u001b[39m W1, W2 = \u001b[38;5;28mself\u001b[39m.params[\u001b[33m\"\u001b[39m\u001b[33mW1\u001b[39m\u001b[33m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m.params[\u001b[33m\"\u001b[39m\u001b[33mW2\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     24\u001b[39m b1, b2 = \u001b[38;5;28mself\u001b[39m.params[\u001b[33m\"\u001b[39m\u001b[33mb1\u001b[39m\u001b[33m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m.params[\u001b[33m\"\u001b[39m\u001b[33mb2\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m a1 = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[43m)\u001b[49m + b1\n\u001b[32m     27\u001b[39m z1 = sigmoid(a1)\n\u001b[32m     28\u001b[39m a2 = np.dot(z1, W2) + b2\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 현재 파일의 절대 경로를 기준으로 부모 디렉토리 설정\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "\n",
    "\n",
    "def print_progress(\n",
    "    iteration,\n",
    "    total,\n",
    "    loss,\n",
    "    accuracy,\n",
    "    start_time,\n",
    "    prefix=\"\",\n",
    "    suffix=\"\",\n",
    "    decimals=1,\n",
    "    bar_length=50,\n",
    "):\n",
    "    \"\"\"\n",
    "    학습 진행률을 보여주는 프로그레스 바\n",
    "    \"\"\"\n",
    "    filled_length = int(round(bar_length * iteration / float(total)))\n",
    "    percents = round(100.0 * iteration / float(total), decimals)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    estimated_total = elapsed_time / (iteration + 1) * total if iteration > 0 else 0\n",
    "    remaining_time = estimated_total - elapsed_time\n",
    "\n",
    "    # 시간 형식 변환\n",
    "    elapsed = str(datetime.utcfromtimestamp(int(elapsed_time)).strftime(\"%H:%M:%S\"))\n",
    "    remaining = str(datetime.utcfromtimestamp(int(remaining_time)).strftime(\"%H:%M:%S\"))\n",
    "\n",
    "    bar = \"█\" * filled_length + \"-\" * (bar_length - filled_length)\n",
    "    print(\n",
    "        f\"\\r{prefix} |{bar}| {percents}% {suffix} Loss: {loss:.4f} Acc: {accuracy:.2f}% ET: {elapsed} ETA: {remaining}\",\n",
    "        end=\"\",\n",
    "    )\n",
    "    if iteration == total:\n",
    "        print()\n",
    "\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 500\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    # 기울기 계산\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    # grad = network.gradient(x_batch, t_batch) # 성능 개선판!\n",
    "\n",
    "    # 매개변수 갱신\n",
    "    for key in (\"W1\", \"b1\", \"W2\", \"b2\"):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    # 정확도 계산\n",
    "    train_acc = network.accuracy(x_batch, t_batch)\n",
    "    if i % 100 == 0:  # 100번째 반복마다 테스트 데이터로 정확도 계산\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "    # 진행상황 출력\n",
    "    print_progress(\n",
    "        i + 1,\n",
    "        iters_num,\n",
    "        loss,\n",
    "        train_acc * 100,\n",
    "        start_time,\n",
    "        prefix=\"Progress:\",\n",
    "        suffix=\"Complete\",\n",
    "    )\n",
    "\n",
    "# 그래프 그리기\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Loss 그래프\n",
    "plt.subplot(1, 2, 1)\n",
    "x = np.arange(len(train_loss_list))\n",
    "plt.plot(x, train_loss_list, label=\"loss\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.ylim(0, 3)\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy 그래프\n",
    "plt.subplot(1, 2, 2)\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label=\"train acc\")\n",
    "plt.plot(x, test_acc_list, label=\"test acc\", linestyle=\"--\")\n",
    "plt.xlabel(\"iterations (x100)\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
